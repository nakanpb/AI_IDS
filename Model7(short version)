#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
payload_cnn_512_multi_memmap_lite.py
- โค้ดแกนสั้นลงจากฉบับเต็ม: memmap -> split (family/group-aware + rebalance) -> train -> choose thr (best_f1) -> test -> save
- ตัด: กราฟ/รายงาน/ฮิสทอรี/จับเวลา/continue-mode/EVAL_ONLY/ROC-AUC/plateau/checkpoint/epoch timer/class weight อัตโนมัติ
"""

import os, re, binascii, base64, random, datetime, math
from pathlib import Path
import numpy as np
import pandas as pd

from sklearn.metrics import (
    classification_report, confusion_matrix,
    average_precision_score, precision_recall_curve, accuracy_score
)

import tensorflow as tf
from tensorflow.keras import mixed_precision
from tensorflow.keras.regularizers import l2
import joblib

# ====== CONFIG (ย่อ) ======
PAYLOAD_COL           = "payload"
LABEL_COL             = "label"
FIXED_LEN             = 512

CSV_FILES = [
    ("/home/intgan/Desktop/jek/zeus(csv)/zeus.csv", "zeus"),
    ("/home/intgan/Desktop/jek/emotet(csv)/emotet(192-3).csv", "emotet"),
    ("/home/intgan/Desktop/jek/emotet(csv)/emotet(264-1).csv", "emotet"),
    ("/home/intgan/Desktop/jek/emotet(csv)/emotet(264-2).csv", "emotet"),
    ("/home/intgan/Desktop/jek/emotet(csv)/emotet(268-1).csv", "emotet"),
    ("/home/intgan/Desktop/jek/emotet(csv)/emotet(269-1).csv", "emotet"),
    ("/home/intgan/Desktop/jek/emotet(csv)/emotet(271-1).csv", "emotet"),
    ("/home/intgan/Desktop/jek/emotet(csv)/emotet(272-1).csv", "emotet"),
    ("/home/intgan/Desktop/jek/emotet(csv)/emotet(276-1).csv", "emotet"),
    ("/home/intgan/Desktop/jek/emotet(csv)/emotet(279-1).csv", "emotet"),
    ("/home/intgan/Desktop/jek/trickbot(csv)/trickbot(238-1).csv", "trickbot"),
    ("/home/intgan/Desktop/jek/trickbot(csv)/trickbot(240-1).csv", "trickbot"),
    ("/home/intgan/Desktop/jek/trickbot(csv)/trickbot(241-1).csv", "trickbot"),
    ("/home/intgan/Desktop/jek/trickbot(csv)/trickbot(247-1).csv", "trickbot"),
    ("/home/intgan/Desktop/jek/trickbot(csv)/trickbot(261-2).csv", "trickbot"),
    ("/home/intgan/Desktop/jek/trickbot(csv)/trickbot(265-1).csv", "trickbot"),
    ("/home/intgan/Desktop/jek/trickbot(csv)/trickbot(327-1).csv", "trickbot"),
    ("/home/intgan/Desktop/jek/benign/realbenign1.csv", "benign"),
    ("/home/intgan/Desktop/jek/benign/realbenign2.csv", "benign"),
    ("/home/intgan/Desktop/jek/benign/realbenign3.csv", "benign"),
    ("/home/intgan/Desktop/jek/benign/realbenign4.csv", "benign"),
    ("/home/intgan/Desktop/jek/benign/realbenign5.csv", "benign"),
]

TEST_SIZE             = 0.20
VAL_SIZE              = 0.10

BATCH                 = 32
EPOCHS                = 15
SEED                  = 42

USE_MIXED_PRECISION   = True   # อยากสั้นสุดให้ตั้ง False แล้วลบบรรทัด mixed_precision ด้านล่างได้
EXPORT_DIR            = "/home/intgan/Desktop/jek/train"
EXPORT_NAME_PREFIX    = "model7_short"
MMAP_DIR              = os.path.join(EXPORT_DIR, "mmap")

# ====== Repro & GPU (คงสั้น ๆ) ======
os.environ["PYTHONHASHSEED"] = str(SEED)
random.seed(SEED); np.random.seed(SEED); tf.random.set_seed(SEED)
gpus = tf.config.list_physical_devices('GPU')
if gpus:
    for gpu in gpus: tf.config.experimental.set_memory_growth(gpu, True)
    if USE_MIXED_PRECISION: mixed_precision.set_global_policy('mixed_float16')
    print(f"[GPU] Found: {gpus} | mixed_precision={USE_MIXED_PRECISION}")
else:
    print("[GPU] ไม่พบการ์ดจอ — จะใช้ CPU")

# ====== Utils (จำเป็น) ======
LABEL_MAP = {'benign':0,'Benign':0,'BENIGN':0,'malicious':1,'Malicious':1,'MALICIOUS':1,0:0,1:1}
b64_re = re.compile(r'^[A-Za-z0-9+/=\s]+$')
hex_re = re.compile(r'^[0-9A-Fa-f\s]+$')

def ts_now(): return datetime.datetime.now().strftime("%Y%m%d-%H%M%S")
def ensure_dir(p): os.makedirs(p, exist_ok=True); return p

def decode_payload(s: str) -> bytes:
    if not isinstance(s, str): return b""
    s = s.strip()
    if not s: return b""
    if b64_re.match(s):
        try: return base64.b64decode(s, validate=True)
        except Exception: pass
    if hex_re.match(s):
        try: return binascii.unhexlify(re.sub(r"\s+","", s))
        except Exception: pass
    return s.encode("latin-1", errors="ignore")

def to_fixed_len_uint8(b: bytes, L=FIXED_LEN, mode="head"):
    arr = np.frombuffer(b, dtype=np.uint8)
    n = len(arr)
    if n >= L:
        if mode == "tail": return arr[-L:]
        if mode == "middle": start = max((n-L)//2, 0); return arr[start:start+L]
        return arr[:L]
    out = np.zeros(L, dtype=np.uint8); out[:n] = arr; return out

# ====== Memmap (คงโครงสร้างหลัก) ======
def _normalize_csvs(csv_files): return [(str(p), str(fam)) for p, fam in csv_files]

def _count_rows_quick(path, chunksize=200_000):
    total = 0
    for ch in pd.read_csv(path, usecols=[PAYLOAD_COL, LABEL_COL],
                          chunksize=chunksize, keep_default_na=False, na_filter=False):
        total += len(ch)
    return total

def build_memmap_from_csvs(csv_files, L=FIXED_LEN, mmap_dir=MMAP_DIR):
    ensure_dir(mmap_dir)
    norm = _normalize_csvs(csv_files)
    print("[MMAP] counting ...")
    total = 0; raw_by = {}; eff_by = {}
    for p, fam in norm:
        n = _count_rows_quick(p); raw_by[fam] = raw_by.get(fam, 0) + n; total += n
        print(f"  - {Path(p).name}: {n} (family={fam})")

    X_path = os.path.join(mmap_dir, "X_all.int32.mmap")
    y_path = os.path.join(mmap_dir, "y_all.float32.mmap")
    g_path = os.path.join(mmap_dir, "groups.int32.mmap")
    f_path = os.path.join(mmap_dir, "family.int32.mmap")

    X_all = np.memmap(X_path, dtype=np.int32,   mode="w+", shape=(total, L))
    y_all = np.memmap(y_path, dtype=np.float32, mode="w+", shape=(total,))
    groups= np.memmap(g_path, dtype=np.int32,   mode="w+", shape=(total,))
    family= np.memmap(f_path, dtype=np.int32,   mode="w+", shape=(total,))

    fam2id, grp2id = {}, {}; w = 0
    print("[MMAP] writing ...")
    for p, fam in norm:
        fam_id = fam2id.setdefault(fam, len(fam2id))
        grp_id = grp2id.setdefault(Path(p).name, len(grp2id))
        for chunk in pd.read_csv(p, usecols=[PAYLOAD_COL, LABEL_COL],
                                 chunksize=200_000, keep_default_na=False, na_filter=False):
            chunk[LABEL_COL] = chunk[LABEL_COL].map(lambda x: LABEL_MAP.get(x, x)).astype(int)
            s = chunk[PAYLOAD_COL].astype(str)
            chunk = chunk[s.str.len() > 0]
            vecs, keep = [], []
            for s in chunk[PAYLOAD_COL].values:
                b = decode_payload(s)
                if len(b) == 0: keep.append(False)
                else:
                    vecs.append(to_fixed_len_uint8(b, L=L, mode="head"))
                    keep.append(True)
            if not vecs: continue
            keep = np.array(keep, bool)
            chunk = chunk[keep]
            arr = np.vstack(vecs).astype(np.int32)
            n = len(chunk)
            X_all[w:w+n] = arr
            y_all[w:w+n] = chunk[LABEL_COL].values.astype(np.float32)
            groups[w:w+n] = grp_id
            family[w:w+n] = fam_id
            w += n
            eff_by[fam] = eff_by.get(fam, 0) + n

    X_all.flush(); y_all.flush(); groups.flush(); family.flush()
    print(f"[MMAP] done: {w}/{total}")
    return {"X_path": X_path, "y_path": y_path, "g_path": g_path, "f_path": f_path,
            "N_total": total, "N_eff": w, "L": L, "fam2id": fam2id, "grp2id": grp2id,
            "raw_rows_by_fam": raw_by, "eff_rows_by_fam": eff_by}

def open_memmaps(paths, N, L):
    X_all  = np.memmap(paths["X_path"], dtype=np.int32,   mode="r", shape=(N, L))
    y_all  = np.memmap(paths["y_path"], dtype=np.float32, mode="r", shape=(N,))
    groups = np.memmap(paths["g_path"], dtype=np.int32,   mode="r", shape=(N,))
    family = np.memmap(paths["f_path"], dtype=np.int32,   mode="r", shape=(N,))
    return X_all, y_all, groups, family

# ====== Split (คงเวอร์ชันเคร่งครัด แต่ย่อการพิมพ์) ======
def enforce_family_coverage(train_idx, val_idx, test_idx, family_arr, min_each=1):
    fams = np.unique(family_arr)
    for fam in fams:
        # TEST
        need = max(0, min_each - int((family_arr[test_idx] == fam).sum()))
        if need > 0:
            src_pool = np.concatenate([train_idx, val_idx])
            cand = src_pool[family_arr[src_pool] == fam]
            take = cand[:need] if len(cand) >= need else cand
            train_idx = train_idx[~np.isin(train_idx, take)]
            val_idx   = val_idx[  ~np.isin(val_idx,   take)]
            test_idx  = np.concatenate([test_idx, take])
        # TRAIN
        need = max(0, min_each - int((family_arr[train_idx] == fam).sum()))
        if need > 0:
            src_pool = np.concatenate([val_idx, test_idx])
            cand = src_pool[family_arr[src_pool] == fam]
            take = cand[:need] if len(cand) >= need else cand
            val_idx   = val_idx[ ~np.isin(val_idx,  take)]
            test_idx  = test_idx[~np.isin(test_idx, take)]
            train_idx = np.concatenate([train_idx, take])
        # VAL
        need = max(0, min_each - int((family_arr[val_idx] == fam).sum()))
        if need > 0:
            src_pool = np.concatenate([train_idx, test_idx])
            cand = src_pool[family_arr[src_pool] == fam]
            take = cand[:need] if len(cand) >= need else cand
            train_idx = train_idx[~np.isin(train_idx, take)]
            test_idx  = test_idx[ ~np.isin(test_idx,  take)]
            val_idx   = np.concatenate([val_idx, take])
    return train_idx, val_idx, test_idx

def split_family_group_aware_quota(groups, family, y,
                                   test_frac=0.20, val_frac=0.10,
                                   min_per_split_per_family=50,
                                   min_per_label_per_family=10,
                                   seed=42):
    rng = np.random.RandomState(seed)
    uniq_groups = np.unique(groups)
    grp2idx = {g: np.where(groups == g)[0] for g in uniq_groups}
    grp2fam = {g: int(np.argmax(np.bincount(family[grp2idx[g]]))) for g in uniq_groups}
    fams = np.unique(family)
    fam2groups = {f: [g for g in uniq_groups if grp2fam[g] == f] for f in fams}
    take_train, take_val, take_test = [], [], []

    for f in fams:
        idx_f = np.where(family == f)[0]
        n_f = len(idx_f)
        if n_f == 0: continue

        n_test = max(int(round(test_frac * n_f)), 1)
        n_val  = max(int(round(val_frac  * n_f)), 1)
        n_train= max(n_f - n_test - n_val, 1)

        if n_f >= 3*min_per_split_per_family:
            n_test  = max(n_test,  min_per_split_per_family)
            n_val   = max(n_val,   min_per_split_per_family)
            n_train = max(n_train, min_per_split_per_family)

        tot = n_train + n_val + n_test
        if tot > n_f:
            scale = n_f / float(tot)
            n_train = max(int(round(n_train*scale)), 1)
            n_val   = max(int(round(n_val  *scale)), 1)
            n_test  = max(int(round(n_test *scale)), 1)
            while (n_train + n_val + n_test) > n_f:
                n_train -= 1

        g_list = fam2groups.get(f, [])
        if len(g_list) >= 2:
            g_list = g_list[:]; rng.shuffle(g_list)
            def pick_by_count(target_n, pool_groups):
                chosen, cnt = [], 0
                for g in pool_groups:
                    chosen.append(g); cnt += len(grp2idx[g])
                    if cnt >= target_n: break
                return chosen
            g_test = pick_by_count(n_test, g_list)
            rest   = [g for g in g_list if g not in set(g_test)]
            g_val  = pick_by_count(n_val, rest)
            rest   = [g for g in rest if g not in set(g_val)]
            g_train= rest
            idx_test_f  = np.concatenate([grp2idx[g] for g in g_test])  if g_test  else np.array([], int)
            idx_val_f   = np.concatenate([grp2idx[g] for g in g_val])   if g_val   else np.array([], int)
            idx_train_f = np.concatenate([grp2idx[g] for g in g_train]) if g_train else np.array([], int)
        else:
            idx_f = idx_f.copy(); rng.shuffle(idx_f)
            idx_test_f  = idx_f[:n_test]
            idx_val_f   = idx_f[n_test:n_test+n_val]
            idx_train_f = idx_f[n_test+n_val:]

        for name, arr in [("val", idx_val_f), ("test", idx_test_f)]:
            y_sub = y[arr]
            need0 = max(0, min_per_label_per_family - int((y_sub == 0).sum()))
            need1 = max(0, min_per_label_per_family - int((y_sub == 1).sum()))
            if (need0 > 0 or need1 > 0) and len(idx_train_f) > 0:
                y_tr = y[idx_train_f]
                move0 = idx_train_f[np.where(y_tr == 0)[0]][:need0]
                move1 = idx_train_f[np.where(y_tr == 1)[0]][:need1]
                move  = np.concatenate([move0, move1]) if (len(move0)+len(move1)) else np.array([], int)
                if name == "val":  idx_val_f  = np.concatenate([idx_val_f,  move])
                else:               idx_test_f = np.concatenate([idx_test_f, move])
                idx_train_f = idx_train_f[~np.isin(idx_train_f, move)]

        take_train.append(idx_train_f); take_val.append(idx_val_f); take_test.append(idx_test_f)

    train_idx = np.unique(np.concatenate(take_train) if take_train else np.array([],int))
    val_idx   = np.unique(np.concatenate(take_val)   if take_val   else np.array([],int))
    test_idx  = np.unique(np.concatenate(take_test)  if take_test  else np.array([],int))

    def disjoint(a,b,c):
        a = np.setdiff1d(a, np.union1d(b,c))
        b = np.setdiff1d(b, np.union1d(a,c))
        c = np.setdiff1d(c, np.union1d(a,b))
        return a,b,c
    train_idx, val_idx, test_idx = disjoint(train_idx, val_idx, test_idx)
    return train_idx.astype(int), val_idx.astype(int), test_idx.astype(int)

def rebalance_split_to_targets(idx_split, family_arr, fam2id, benign_name="benign",
                               keep_floor_per_family=1, seed=42,
                               oversample=False, max_multiplier=3,
                               targets=None):
    rng = np.random.RandomState(seed)
    inv = {v: k for k, v in fam2id.items()}
    fam_ids_in_split = np.unique(family_arr[idx_split])
    if len(idx_split) == 0: return np.sort(idx_split.astype(int))
    fam2indices = {int(fid): idx_split[family_arr[idx_split] == int(fid)]
                   for fid in fam_ids_in_split}
    N = len(idx_split)

    # quota
    quota = {}
    if targets is not None:
        s = float(sum(targets.values()))
        if s <= 0: return np.sort(idx_split.astype(int))
        for name, p in targets.items():
            p_norm = float(p) / s
            fid_match = None
            for fid in fam_ids_in_split:
                if inv[int(fid)].lower() == name.lower():
                    fid_match = int(fid); break
            if fid_match is not None:
                quota[fid_match] = max(int(round(p_norm * N)), 0)
        gap = N - sum(quota.values())
        if gap != 0 and len(quota) > 0:
            keys = list(quota.keys()); i = 0
            while gap != 0 and len(keys) > 0:
                k = keys[i % len(keys)]; quota[k] += 1 if gap > 0 else -1
                if quota[k] < 0: quota[k] = 0
                gap += (-1 if gap > 0 else 1); i += 1
    else:
        benign_id = None
        for fid in fam_ids_in_split:
            if inv[int(fid)].lower() == benign_name.lower():
                benign_id = int(fid); break
        if benign_id is None:
            per = N // max(1, len(fam_ids_in_split))
            for fid in fam_ids_in_split: quota[int(fid)] = per
            for fid in list(quota.keys())[:(N - sum(quota.values()))]: quota[fid] += 1
        else:
            non_benign = [int(fid) for fid in fam_ids_in_split if int(fid) != benign_id]
            target_benign = int(round(0.50 * N)); target_rest = N - target_benign
            quota[benign_id] = target_benign
            if len(non_benign) > 0:
                per = target_rest // len(non_benign)
                for fid in non_benign: quota[fid] = per
                rem = target_rest - per * len(non_benign)
                for fid in non_benign[:rem]: quota[fid] += 1

    chosen = []
    for fid, q in quota.items():
        cur = fam2indices.get(int(fid), np.array([], dtype=int))
        n   = len(cur)
        if q <= 0: continue
        take = min(n, q)
        if take > 0: chosen.append(rng.choice(cur, size=take, replace=False))
        need_more = q - take
        if need_more > 0 and oversample and n > 0:
            max_extra = max(0, n * max_multiplier - n)
            add_n = min(need_more, max_extra)
            if add_n > 0: chosen.append(rng.choice(cur, size=add_n, replace=True))

    final_idx = np.concatenate(chosen) if len(chosen) else np.array([], dtype=int)
    if len(final_idx) > N:
        final_idx = rng.choice(final_idx, size=N, replace=False)
    elif len(final_idx) < N:
        pool = idx_split; need = N - len(final_idx)
        if len(pool) > 0:
            extra = rng.choice(pool, size=min(need, len(pool)), replace=False)
            final_idx = np.concatenate([final_idx, extra])
    return np.sort(final_idx.astype(int))

# ====== tf.data ======
def make_dataset_from_indices(X, y, indices, batch, training=False, seed=SEED):
    idx = np.array(indices, dtype=np.int64)
    def gen():
        for i in idx: yield X[i], y[i]
    ds = tf.data.Dataset.from_generator(
        gen,
        output_signature=(tf.TensorSpec(shape=(FIXED_LEN,), dtype=tf.int32),
                          tf.TensorSpec(shape=(), dtype=tf.float32))
    )
    if training:
        ds = ds.shuffle(8192, seed=seed, reshuffle_each_iteration=True).repeat()
    return ds.batch(batch).prefetch(tf.data.AUTOTUNE)

# ====== Threshold (best_f1 เท่านั้น) ======
def choose_threshold_best_f1(y_true, proba, min_thr=1e-6):
    prec, rec, thr = precision_recall_curve(y_true, proba)  # len(thr)=len(prec)-1
    f1 = 2*prec[1:]*rec[1:] / (prec[1:]+rec[1:]+1e-9)
    i  = int(np.argmax(f1))
    t  = max(float(thr[i]), min_thr)
    return t, {"policy":"best_f1","F1":float(f1[i]),
               "Precision":float(prec[i+1]),"Recall":float(rec[i+1])}

# ====== Model (เหมือนเดิมแต่กระชับ) ======
def make_model(L=FIXED_LEN, emb_dim=16, filters=128, k=7, drop=0.25):
    inp = tf.keras.Input(shape=(L,), dtype=tf.int32)
    x = tf.keras.layers.Embedding(input_dim=256, output_dim=emb_dim)(inp)
    def block(x, dilation):
        x = tf.keras.layers.Conv1D(filters, k, padding="same", dilation_rate=dilation,
                                   kernel_regularizer=l2(1e-5))(x)
        x = tf.keras.layers.BatchNormalization()(x)
        x = tf.keras.layers.Activation("relu")(x)
        return tf.keras.layers.Dropout(drop)(x)
    for d in [1,2,4]:
        x = block(x, d)
    x = tf.keras.layers.GlobalMaxPooling1D()(x)
    x = tf.keras.layers.Dense(256, activation="relu", kernel_regularizer=l2(1e-5))(x)
    x = tf.keras.layers.Dropout(0.35)(x)
    out = tf.keras.layers.Dense(1, activation="sigmoid", dtype='float32')(x)
    return tf.keras.Model(inp, out)

# ====== MAIN (ย่อ logic) ======
def main():
    if len(CSV_FILES) == 0: raise SystemExit("กรุณากำหนด CSV_FILES ก่อน")
    ensure_dir(EXPORT_DIR); ensure_dir(MMAP_DIR)
    ts = ts_now()

    # -- MMAP
    mmap_info = build_memmap_from_csvs(CSV_FILES, L=FIXED_LEN, mmap_dir=MMAP_DIR)
    N_eff = int(mmap_info["N_eff"])
    X_all, y_all, groups, family = open_memmaps(mmap_info, N_eff, mmap_info["L"])
    fam2id = mmap_info["fam2id"]

    # -- Split (family/group-aware + enforce + rebalance)
    train_idx, val_idx, test_idx = split_family_group_aware_quota(
        groups=groups, family=family, y=y_all,
        test_frac=TEST_SIZE, val_frac=VAL_SIZE,
        min_per_split_per_family=max(20, int(0.005*len(y_all))),
        min_per_label_per_family=10, seed=SEED
    )
    train_idx, val_idx, test_idx = enforce_family_coverage(train_idx, val_idx, test_idx, family, min_each=1)

    train_idx = rebalance_split_to_targets(
        train_idx, family, fam2id,
        benign_name="benign", keep_floor_per_family=50, seed=SEED,
        oversample=True, max_multiplier=3
    )
    val_idx = rebalance_split_to_targets(
        val_idx, family, fam2id,
        benign_name="benign", keep_floor_per_family=10, seed=SEED, oversample=False
    )
    test_idx = rebalance_split_to_targets(
        test_idx, family, fam2id,
        benign_name="benign", keep_floor_per_family=10, seed=SEED, oversample=False
    )

    print("Shapes:", (len(train_idx), FIXED_LEN), (len(val_idx), FIXED_LEN), (len(test_idx), FIXED_LEN))

    # -- Datasets
    train_ds = make_dataset_from_indices(X_all, y_all, train_idx, BATCH, training=True)
    val_ds   = make_dataset_from_indices(X_all, y_all, val_idx,   BATCH)
    test_ds  = make_dataset_from_indices(X_all, y_all, test_idx,  BATCH)

    # -- Build & compile (metrics เฉพาะ PR-AUC + Precision/Recall)
    model = make_model()
    optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4, clipvalue=1.0)
    model.compile(
        optimizer=optimizer,
        loss="binary_crossentropy",
        metrics=[
            tf.keras.metrics.AUC(name="PR-AUC", curve="PR"),
            tf.keras.metrics.Precision(name="Precision"),
            tf.keras.metrics.Recall(name="Recall"),
        ],
    )
    model.summary()

    # -- Train (callback เดียวพอ)
    earlystop_cb = tf.keras.callbacks.EarlyStopping(
        monitor="val_PR-AUC", mode="max", patience=5, restore_best_weights=True
    )
    steps_per_epoch = math.ceil(len(train_idx)/BATCH)
    history = model.fit(
        train_ds,
        validation_data=val_ds,
        epochs=EPOCHS,
        steps_per_epoch=steps_per_epoch,
        callbacks=[earlystop_cb],
        verbose=1
    )

    # -- VAL -> choose threshold
    proba_val = model.predict(val_ds).ravel()
    chosen_thr, thr_info = choose_threshold_best_f1(y_all[val_idx], proba_val, min_thr=1e-6)

    # -- TEST evaluate
    proba_test = model.predict(test_ds).ravel()
    pred_test  = (proba_test >= chosen_thr).astype(int)
    y_true = y_all[test_idx].astype(int)
    pr  = average_precision_score(y_true, proba_test)
    cm  = confusion_matrix(y_true, pred_test)
    report_dict = classification_report(y_true, pred_test, digits=4, output_dict=True, zero_division=0)
    acc = accuracy_score(y_true, pred_test)

    print("\n==== FINAL EVALUATION (LITE) ====")
    print(f"PR-AUC (TEST): {pr:.4f}")
    print(f"[THRESHOLD from VAL] = {chosen_thr:.6f}  info = {thr_info}")
    print("Classification Report (per class):")
    print(classification_report(y_true, pred_test, digits=4, zero_division=0))
    print(f"Confusion Matrix (TEST) @thr={chosen_thr:.6f}:\n{cm}")
    print(f"[ACCURACY] {acc:.6f}")

    # -- Save model + compact meta
    final_model_path = os.path.join(EXPORT_DIR, f"{EXPORT_NAME_PREFIX}.final.{ts}.keras")
    model.save(final_model_path)
    meta = {
        "fixed_len": FIXED_LEN, "payload_col": PAYLOAD_COL, "label_col": LABEL_COL,
        "decode": "base64->hex->latin1", "window_mode": "head",
        "seed": SEED, "timestamp": ts,
        "files": [(str(p), str(fam)) for p, fam in _normalize_csvs(CSV_FILES)],
        "mmap_dir": MMAP_DIR, "fam2id": mmap_info["fam2id"], "grp2id": mmap_info["grp2id"],
        "N_total": int(mmap_info["N_total"]), "N_eff": int(mmap_info["N_eff"]),
        "indices": {"train": train_idx.tolist(), "val": val_idx.tolist(), "test": test_idx.tolist()},
        "threshold": float(chosen_thr), "threshold_info": thr_info,
        "metrics": {"pr_auc_test": float(pr),
                    "classification_report": report_dict,
                    "confusion_matrix": cm.tolist(),
                    "accuracy": float(acc)},
        "export": {"model": final_model_path}
    }
    meta_path = os.path.join(EXPORT_DIR, f"preprocess_meta.{ts}.joblib")
    joblib.dump(meta, meta_path)
    print(f"Saved FINAL -> {final_model_path}\nSaved META  -> {meta_path}")

    return final_model_path, meta_path

# ====== Inference helper (คงไว้ ใช้สะดวก) ======
def predict_payload_string(s, model_path, meta_path=None, fixed_len=FIXED_LEN, window_mode="head",
                           threshold=None):
    mdl = tf.keras.models.load_model(model_path)
    if meta_path:
        m = joblib.load(meta_path)
        fixed_len   = m.get("fixed_len", fixed_len)
        window_mode = m.get("window_mode", window_mode)
        if threshold is None:
            threshold = float(m.get("threshold", 0.5))
    b = decode_payload(s)
    x = to_fixed_len_uint8(b, L=fixed_len, mode=window_mode).astype(np.int32)
    p = float(mdl.predict(x[None, ...], verbose=0).ravel()[0])
    thr = 0.5 if threshold is None else float(threshold)
    return {"prob_malicious": p, f"label@{thr:g}": int(p >= thr), "threshold_used": thr}

if __name__ == "__main__":
    final_model_path, meta_path = main()
    demo_hex = "48656c6c6f"  # "Hello"
    print("Demo inference:", predict_payload_string(demo_hex, model_path=final_model_path, meta_path=meta_path))
